#+TITLE: plz Notes

* Contents
:PROPERTIES:
:TOC:      :include siblings :depth 1 :ignore this
:END:
:CONTENTS:
- [[#tasks][Tasks]]
- [[#ideas][Ideas]]
- [[#api-design][API Design]]
- [[#references][References]]
  - [[#similar-projects][Similar projects]]
:END:

* Tasks

** DONE Ensure that secrets are not leaked via command line or temp files
CLOSED: [2021-08-15 Sun 15:34]
:LOGBOOK:
-  State "DONE"       from "TODO"       [2021-08-15 Sun 15:34]
:END:

e.g. =request.el= can leak secrets and other data via the command line and [[https://github.com/tkf/emacs-request/blob/431d14343c61bc51a86c9a9e1acb6c26fe9a6298/request.el#L709][leftover temp files]].  We want to handle this safely.

[2021-08-15 Sun 15:33]  Finally figured out how to do this using ~--config~.  It required some trial-and-error, since the curl man page doesn't explain how to pass request bodies over STDIN after the arguments.  But it works!

*** Copied note from Ement.el notes

There seem to be two options: pass the URL on the command line, or pass it in a temp file.  Either way is bad: the command line makes it visible to all users (AFAIK), and temp files are messy, could be left behind, clutter the disk, etc.

Curl has so many options that I was hoping for a way to pass the URL via STDIN, and there is, but that appears to preclude the passing of other data via STDIN.  I found [[https://curl.se/mail/archive-2003-08/0099.html][this mailing list thread from 2003]] where Rich Gray asks for this very feature, but Daniel Stenberg shoots down the idea:

#+BEGIN_QUOTE
While you of course are 100% correct, I fail to see why curl has to do all this by itself. This kind of magic will only be attempted by people who are using unix(-like) operating systems and if you sit in front of a unix box, it would be dead easy to write a wrapper script around curl that hides all the arguments quite nicely already, right?

The same goes for your idea of being able to read from specific file handle numbers.

I don't think adding these features would benefit more than a few unix hackers (most likely wearing beards! ;-O), who already know how to overcome the problems they fix.
#+END_QUOTE

In fact, writing a wrapper script does not help at all: how horribly hacky and messy it would be to /write a shell script to the disk every time I want to call curl from Emacs/.

[2021-09-24 Fri]  This is done in =plz= now.

* Ideas

** TODO Use finalizers to clean up response buffers
:LOGBOOK:
-  State "TODO"       from              [2020-10-30 Fri 12:58]
:END:

+  [[info:elisp#Finalizer%20Type][info:elisp#Finalizer Type]]

This might allow us to avoid or delay putting the response body in a string, which could improve performance.

** Queue

+ [[https://github.com/alphapapa/plz.el/tree/wip/queue][Branch: wip/queue]]

*** [[https://github.com/alphapapa/plz.el/commit/3469bcdbb2e2c1a772ecadcf4f50da317065a96d#commitcomment-71388831][Feedback from Chris Wellons]]

#+begin_example markdown
  Going from 0 dependencies to 1 dependency is a big jump. With zero you get
  some nice benefits, like never worrying about a package system (esp. when
  testing, etc.). If you're going to make that transition it better be worth
  the cost. A linked list queue is not worth it.

  While that ELPA queue package is decent enough, this is trivial
  functionality. You can implement an equivalent queue in just a few lines
  of code. For example, here's a minimalistic one built out of a cons:

  https://github.com/skeeto/emacs-aio/blob/master/aio.el#L322

  (Feel free to copy this one if you like. It's in the public domain, and I
  don't even care if you give me credit since it's so trivial. "A little
  copying is better than a little dependency.")

  plz-clear and plz-reset: Don't immediately nil the active list. A request
  is not properly retired until all the callbacks have completed, and
  requests should remain in the active list until then. That probably also
  means blocking clear/reset until the active list clears. Otherwise 1) the
  caller might observe an empty queue with invisible still-active requests,
  which isn't really a valid state, and 2) it's up to the caller to somehow
  wait (complicated and error-prone) if needed for the queue to return to a
  valid state. You probably need to track this "cancellation" state so that
  your wrapper callbacks don't actually run the queue, and so you can
  potentially catch/handle callbacks enqueueing while you're busy trying to
  clear the active list.

  For plz-clear: queued, inactive requests should also have their :else
  callbacks invoked to indicate they're not going to happen. Perhaps some
  kind of "cancelled" error?

  For plz-reset: I'm not sure this is really even a sound idea. The request
  is concurrent, and it may still complete on the server side, including any
  side effects, despite killing the process. These requests should not be
  retried unless the caller explicitly requeues them (they know the context
  but plz does not), and they'd know to do so because you reported to :else
  that it was canceled.

  I've probably said it before, but rigid guarantees around callbacks are
  super important. In order to build anything reliable on this, callbacks
  must be called exactly the right number of times (i.e. exactly once, not
  zero or twice) at the right time, and the invariants must hold around
  these callbacks (i.e. the queue state makes sense during the callback).
  Fundamentally this is a concurrency situation even if there are no
  explicit threads/coroutines involved. The biggest flaw with Emacs' own
  url-retrieve, and the primary reason it's so unreliable, is its poor,
  unpredictable callback discipline.

  So a rule: When something goes in the queue, it stays there until plz has
  informed its callbacks of the results. The callbacks on an enqueued
  request are never called twice (for that request): it either fails or
  succeeds and that's it.

  There's a warning about signals in callbacks aborting queue processing,
  but I'd just make the queue robust regardless. Let the signal unwind to
  the top-level and make noise, but keep the queue moving. A mistake will
  eventually happen, and then some consumer of this library will end up in
  an invalid state (e.g. waiting on a queue that's not running because of an
  unexpected signal). Example: There are still very rare edge cases in
  Elfeed I haven't caught (I suspect there's one related to DNS failures),
  and once every few months or so I have to use the emergency elfeed-unjam
  lever to reset the queue to a good state.

  plist-put is destructive, but you must still capture the return value,
  particularly when the property doesn't exist yet. (Imagine setting a
  property on a nil list.)

  Is "delete" the right function for removing items? This uses "equal" but
  you probably want eq like delq. cl-delete more sensibly uses "eql" by
  default, which is just as good.

  Some nit-picky stuff that probably doesn't matter, but I can't help
  commenting since I'm (overly) sensitive to pessimization:

  ,* Using "delete" on the active list is (almost) quadratic time, or more
  accurately, O(n*m) for n requests and a limit of m. Using a set (read:
  hashtable) or some other kind of O(1) removal would bring this down to
  linear, O(n), time. I'm putting this under nit-picking since the limit is
  likely a small number.

  ,* The use of "length" in plz-run is O(n*m) time just like delete. You can
  avoid this by tracking the length as separate counter rather than walking
  the list in order to count. Alternatively, this is automatically fixed
  when you swap in a set, since presumably it has an O(1) length function.

  ,* Similarly, I don't like the recursion in plz-run even though it's also
  bounded by the limit. Unless Emacs got TCO when I wasn't looking, I'd a
  loop just to be more careful. (Go recently ran into trouble parsing PEM
  using recursion despite having massive call stacks.)

  Ending on a positive note: I like that you consistently return the queue
  object. (Except for plz-run?)
#+end_example

** Name

+  =plz=
     -  The current name.
+  =curly=
     -  Since the library is based on curl, it wouldn't be a bad idea to have =curl= in the name, and this isn't too long.

* API Design

** Async

Some sample cases that the API should make easy.

*** Body as string

#+BEGIN_SRC elisp
  (plz-get url
    :with 'body-string
    :then (lambda (body-string)
            (setf something body-string)))
#+END_SRC

*** Body as buffer

#+BEGIN_SRC elisp
  ;; Decodes body and narrows buffer to it.
  (plz-get url
    :with 'buffer
    :then (lambda (buffer)
            (with-current-buffer buffer
              (setf text (buffer-substring (point-min) (point-max))))))
#+END_SRC

#+BEGIN_SRC elisp
  ;; Narrows buffer to undecoded body, e.g. for binary files.
  (plz-get url
    :with 'buffer-undecoded  ; `buffer-binary'?
    :then (lambda (buffer)
            (with-current-buffer buffer
              (setf binary-content (buffer-substring (point-min) (point-max))))))
#+END_SRC

**** Callback with point at body start
:PROPERTIES:
:ID:       1795462e-01bc-4f0b-97ab-3c1b2e75485c
:END:

Assuming that =plz= has already called =decode-coding-region=, this is straightforward, but the caller shouldn't have to do this extra work.

#+BEGIN_SRC elisp
  (plz-get url
    :then (lambda (buffer)
            (buffer-substring (point) (point-max))))
#+END_SRC

*** Body parsed with function

#+BEGIN_SRC elisp
  ;; Narrows buffer to body, decodes it, calls callback with result of `json-read'.
  (plz-get url
    :with #'json-read
    :then (lambda (json)
            (setf something (alist-get 'key json))))
#+END_SRC

#+BEGIN_SRC elisp
  ;; Narrows buffer to body, decodes it, parses with
  ;; `libxml-parse-html-region', calls callback with DOM.
  (plz-get url
    :with (lambda ()
            (libxml-parse-html-region (point-min) (point-max) url))
    :then (lambda (dom)
            (with-current-buffer (generate-new-buffer "*plz-browse*")
              (shr-insert-document dom))))
#+END_SRC

*** HTTP response with headers

* References
:PROPERTIES:
:TOC:      :depth 1
:END:

** Discussions

*** [[https://lists.gnu.org/archive/html/emacs-devel/2016-12/msg01070.html][with-url]]

[2020-12-20 Sun 08:11]  At the end of 2016, Lars Ingebrigtsen [[https://lists.gnu.org/archive/html/emacs-devel/2016-12/msg01070.html][proposed]] a ~with-url~ macro that improves on ~url-retrieve~ and ~url-retrieve-synchronously~.  It was [[https://lists.gnu.org/archive/html/emacs-devel/2020-12/msg01220.html][mentioned]] by David Engster in [[https://lists.gnu.org/archive/html/emacs-devel/2020-12/msg01217.html][this thread]] from 2020.  It looks like it has a nice API.  Unfortunately it hasn't been merged.

*** TODO Feedback from Chris Wellons :ATTACH:
:PROPERTIES:
:ID:       975f77fa-5233-4b26-970b-e0d64f2aa950
:Attachments: https%3A%2F%2Fgithub.com%2Falphapapa%2Fplz.el%2Fcommit%2F0a860d94dcbb103d05f3ee006772a568904fa4de%23commitcomment-55151841--n1jasq.tar.xz https%3A%2F%2Fgithub.com%2Falphapapa%2Fplz.el%2Fcommit%2F7c27e4bdcd747f0bfc5a6298040739562a941e08%23r55075010--WNr6Ey.tar.xz
:END:
:LOGBOOK:
-  State "TODO"       from              [2021-08-20 Fri 05:37]
:END:

+  [[https://github.com/alphapapa/plz.el/commit/7c27e4bdcd747f0bfc5a6298040739562a941e08#r55075010][Change: Sync with accept-process-output 路 alphapapa/plz.el@7c27e4b 路 GitHub]]
+  [[https://github.com/alphapapa/plz.el/commit/0a860d94dcbb103d05f3ee006772a568904fa4de#commitcomment-55151841][Tests: "There be dragons." 路 alphapapa/plz.el@0a860d9 路 GitHub]]

** Bug reports

*** Emacs Process-related bugs

**** [[https://debbugs.gnu.org/cgi/bugreport.cgi?bug=49897][#49897 - 28.0.50; {PATCH} Make sense of url-retrieve-synchronously - GNU bug report logs]]

**** [[https://debbugs.gnu.org/cgi/bugreport.cgi?bug=49682][#49682 - 27.2.50; accept-process-output within accept-process-output hangs emacs - GNU bug report logs]]

**** [[https://debbugs.gnu.org/cgi/bugreport.cgi?bug=33018][#33018 - 26.1.50; thread starvation with async processes and accept-process-output - GNU bug report logs]]

**** [[https://debbugs.gnu.org/cgi/bugreport.cgi?bug=24201][#24201 - 25.1.50; TLS connections sometimes hang - GNU bug report logs]]

** Similar projects

*** [[https://github.com/ebpa/fetch.el][GitHub - ebpa/fetch.el: A simple HTTP request library modeled after the web browser API]]

A nice-looking wrapper for =url.el=.

