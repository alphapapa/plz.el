#+TITLE: plz Notes

* Contents
:PROPERTIES:
:TOC:      :include siblings :depth 1 :ignore this
:END:
:CONTENTS:
- [[#tasks][Tasks]]
- [[#ideas][Ideas]]
- [[#api-design][API Design]]
- [[#references][References]]
  - [[#similar-projects][Similar projects]]
:END:

* Tasks
:PROPERTIES:
:ID:       bc93ae30-b483-4113-977f-16bb55e6c73c
:END:

** TODO [#B] Use ~with-local-quit~ as appropriate
:LOGBOOK:
- State "TODO"       from              [2023-03-02 Thu 16:18]
:END:

We've been noticing this happening in hyperdrive.el sometimes.  It may be necessary to use ~with-local-quit~ somewhere in =plz='s code.  See: [[id:57a9bbd4-9b6e-4db0-a293-a099d4a05c89][Blocking call to accept-process-output with quit inhibited!! [11 times]​]].

** TODO Handle other process statuses in ~plz--sentinel~
:PROPERTIES:
:milestone: 0.3
:END:
:LOGBOOK:
- State "TODO"       from              [2022-09-18 Sun 11:55]
:END:

In the ~pcase-exhaustive~ form, need to handle ~interrupt\n~, and should have a failsafe as well.

** DONE Run httpbin in GitHub CI for testing
:LOGBOOK:
- State "DONE"       from "TODO"       [2023-06-29 Thu 06:25] \\
  This is now on master.
- State "TODO"       from              [2023-06-25 Sun 22:25]
:END:

+ The httpbin.org site is regularly overloaded, returning 504 and other errors, making the CI useless.  But running the ~kennethreitz/httpbin~ Docker image locally works well, so it should solve the problem on CI as well.
+ The question is how to properly set up the ~test.yml~ file to run the image on GitHub CI.  Locally, I just do ~docker pull kennethreitz/httpbin~ and then ~docker run -p 80 kennethreitz/httpbin~, and set ~plz-test-uri-prefix~ to point to ~localhost~, and it works.  But I expect that there's a more idiomatic way to do that in GitHub CI than to run those commands manually.  Let's see if I can find out...
  + [[https://docs.github.com/en/actions/creating-actions/creating-a-docker-container-action#writing-the-action-code][This page]] seems to cover at least some of it, but I don't want to make a custom "action" if I don't have to.  Surely there's already one for httpbin...
  + The [[https://github.com/postmanlabs/httpbin][httpbin repo page]] doesn't seem to have one in the readme, but maybe it's mentioned elsewhere.  I can't be the first to want to do this.
  + Interestingly, [[https://github.com/postmanlabs/httpbin/pull/637][this PR]] purports to reduce the Docker image's size from 534 MB to 287 MB by basing it on Alpine instead of Ubuntu.  Looking at its Dockerfile, it ends up by just doing ~pip3 install --on-cache-dir /httpbin~.  Maybe I don't even need to use Docker; maybe I can just install the Python package and run it that way.  Unfortunately, the readme's only instructions say to use Docker.
  + [[https://github.com/postmanlabs/httpbin/issues/703][This issue]] also links to some alternatives to httpbin.
    + [[https://github.com/mccutchen/go-httpbin][GitHub - mccutchen/go-httpbin: A reasonably complete and well-tested golang port of httpbin, with zero dependencies outside the go stdlib.]] looks like it might be a good alternative.  Maybe I can just run its Docker image: ~docker run -P mccutchen/go-httpbin~.  The [[https://hub.docker.com/r/mccutchen/go-httpbin/][Docker image page]] has better directions.
      + Looks like this command should work: ~docker run -e PORT=80 -p 80:80 -P mccutchen/go-httpbin~.  Let's try it...
      + Looks like it can work, but ~go-httpbin~ returns slightly different results.  So rather than updating all of the tests to fix that, let's just try using the Python one...
      + And that works!  Now if I tidy up what checkdoc is complaining about, the whole lint/test pass should be green...
      + Yep, everything passes, all lints and tests (except that on Emacs 29, some indentation settings changed, which causes the check-indent pass to fail).

** DONE Ensure that secrets are not leaked via command line or temp files
CLOSED: [2021-08-15 Sun 15:34]
:LOGBOOK:
-  State "DONE"       from "TODO"       [2021-08-15 Sun 15:34]
:END:

e.g. =request.el= can leak secrets and other data via the command line and [[https://github.com/tkf/emacs-request/blob/431d14343c61bc51a86c9a9e1acb6c26fe9a6298/request.el#L709][leftover temp files]].  We want to handle this safely.

[2021-08-15 Sun 15:33]  Finally figured out how to do this using ~--config~.  It required some trial-and-error, since the curl man page doesn't explain how to pass request bodies over STDIN after the arguments.  But it works!

*** Copied note from Ement.el notes

There seem to be two options: pass the URL on the command line, or pass it in a temp file.  Either way is bad: the command line makes it visible to all users (AFAIK), and temp files are messy, could be left behind, clutter the disk, etc.

Curl has so many options that I was hoping for a way to pass the URL via STDIN, and there is, but that appears to preclude the passing of other data via STDIN.  I found [[https://curl.se/mail/archive-2003-08/0099.html][this mailing list thread from 2003]] where Rich Gray asks for this very feature, but Daniel Stenberg shoots down the idea:

#+BEGIN_QUOTE
While you of course are 100% correct, I fail to see why curl has to do all this by itself. This kind of magic will only be attempted by people who are using unix(-like) operating systems and if you sit in front of a unix box, it would be dead easy to write a wrapper script around curl that hides all the arguments quite nicely already, right?

The same goes for your idea of being able to read from specific file handle numbers.

I don't think adding these features would benefit more than a few unix hackers (most likely wearing beards! ;-O), who already know how to overcome the problems they fix.
#+END_QUOTE

In fact, writing a wrapper script does not help at all: how horribly hacky and messy it would be to /write a shell script to the disk every time I want to call curl from Emacs/.

[2021-09-24 Fri]  This is done in =plz= now.

* Ideas

** TODO Use finalizers to clean up response buffers
:LOGBOOK:
-  State "TODO"       from              [2020-10-30 Fri 12:58]
:END:

+  [[info:elisp#Finalizer%20Type][info:elisp#Finalizer Type]]

This might allow us to avoid or delay putting the response body in a string, which could improve performance.

** TODO Experiment with running callbacks with timers
:LOGBOOK:
- State "TODO"       from              [2023-03-14 Tue 11:58]
:END:

+ See: [[id:bbf01f92-1a55-4b86-a92b-f7ef0ed6ad4a][continuation passing in Emacs vs. JUST-THIS-ONE]].

** TODO Allow sending files from disk as request body
:LOGBOOK:
- State "TODO"       from              [2023-03-14 Tue 05:37]
:END:

+ See: [[id:c0012a30-0d08-4b48-8d3a-89d1f3deec20][Sending should not rely on UNIX pipes -- it's slow. · Issue #207 · tkf/emacs-request · GitHub]].

** HTTP status code and error handling improvements

*** Return values to THEN and ELSE functions
As it stands, handling specific errors, like different HTTP status codes or Curl error codes, is awkward.  It requires passing an ELSE function which receives a ~plz-error~ struct which contains any Curl error code in one slot, and any HTTP response in another slot; and accessing the HTTP status code requires destructuring both the ~plz-error~ struct and the contained ~plz-response~ struct.

It becomes even more awkward if various 2xx HTTP codes need to be handled differently, requiring the user to destructure the response and case the status code in both the THEN and ELSE functions.

A potential improvement would be, when the user specifies ~:as 'response~, to return the ~plz-response~ struct to the THEN function even for non-2xx HTTP status codes, which would allow the user to handle all HTTP status codes in a single place.

For the case that Curl returns an error (e.g. connection failure), there would be basically two choices: a) return the ~plz-response~ struct anyway, but using the <100 Curl error code as the status code (which seems like an elegant hack, but a hack nonetheless); or b) return a ~plz-error~ struct instead (which would require the user to typecase the argument in the THEN function, which it would be preferable to avoid).

*** Wrap ~(funcall then ...)~ in ~condition-case~ and pass Lisp error to ELSE
Another potential improvement might be to wrap more of ~plz--sentinel~'s body in a ~condition-case~, to catch any error from the THEN function, and to call the ELSE function with the Lisp error as the value.  Then the ELSE function could dispatch on the type of error and even re-signal it if it's unexpected.

In this case, probably the ELSE function would always receive a Lisp error list as its argument rather than a ~plz-error~ struct.

Of course, this would be a breaking change, so it would have to be considered carefully and, if possible, handled with a graceful transition.

** Parallel fetching
:PROPERTIES:
:ID:       96e3f880-4df4-4f9b-8d9d-fbd04e1eec6e
:END:

See, e.g. Daniel Mendler's [[https://github.com/minad/osm/commit/1264c3e1dc514567a5093b46fa5b4a7abdf74dec][implementation in osm.el]].

** Name

+  =plz=
     -  The current name.
+  =curly=
     -  Since the library is based on curl, it wouldn't be a bad idea to have =curl= in the name, and this isn't too long.

** DONE Queue
:LOGBOOK:
- State "DONE"       from              [2022-07-18 Mon 09:32]
:END:

+ [[https://github.com/alphapapa/plz.el/tree/wip/queue][Branch: wip/queue]]

*** [[https://github.com/alphapapa/plz.el/commit/3469bcdbb2e2c1a772ecadcf4f50da317065a96d#commitcomment-71388831][Feedback from Chris Wellons]]

#+begin_example markdown
  Going from 0 dependencies to 1 dependency is a big jump. With zero you get
  some nice benefits, like never worrying about a package system (esp. when
  testing, etc.). If you're going to make that transition it better be worth
  the cost. A linked list queue is not worth it.

  While that ELPA queue package is decent enough, this is trivial
  functionality. You can implement an equivalent queue in just a few lines
  of code. For example, here's a minimalistic one built out of a cons:

  https://github.com/skeeto/emacs-aio/blob/master/aio.el#L322

  (Feel free to copy this one if you like. It's in the public domain, and I
  don't even care if you give me credit since it's so trivial. "A little
  copying is better than a little dependency.")

  plz-clear and plz-reset: Don't immediately nil the active list. A request
  is not properly retired until all the callbacks have completed, and
  requests should remain in the active list until then. That probably also
  means blocking clear/reset until the active list clears. Otherwise 1) the
  caller might observe an empty queue with invisible still-active requests,
  which isn't really a valid state, and 2) it's up to the caller to somehow
  wait (complicated and error-prone) if needed for the queue to return to a
  valid state. You probably need to track this "cancellation" state so that
  your wrapper callbacks don't actually run the queue, and so you can
  potentially catch/handle callbacks enqueueing while you're busy trying to
  clear the active list.

  For plz-clear: queued, inactive requests should also have their :else
  callbacks invoked to indicate they're not going to happen. Perhaps some
  kind of "cancelled" error?

  For plz-reset: I'm not sure this is really even a sound idea. The request
  is concurrent, and it may still complete on the server side, including any
  side effects, despite killing the process. These requests should not be
  retried unless the caller explicitly requeues them (they know the context
  but plz does not), and they'd know to do so because you reported to :else
  that it was canceled.

  I've probably said it before, but rigid guarantees around callbacks are
  super important. In order to build anything reliable on this, callbacks
  must be called exactly the right number of times (i.e. exactly once, not
  zero or twice) at the right time, and the invariants must hold around
  these callbacks (i.e. the queue state makes sense during the callback).
  Fundamentally this is a concurrency situation even if there are no
  explicit threads/coroutines involved. The biggest flaw with Emacs' own
  url-retrieve, and the primary reason it's so unreliable, is its poor,
  unpredictable callback discipline.

  So a rule: When something goes in the queue, it stays there until plz has
  informed its callbacks of the results. The callbacks on an enqueued
  request are never called twice (for that request): it either fails or
  succeeds and that's it.

  There's a warning about signals in callbacks aborting queue processing,
  but I'd just make the queue robust regardless. Let the signal unwind to
  the top-level and make noise, but keep the queue moving. A mistake will
  eventually happen, and then some consumer of this library will end up in
  an invalid state (e.g. waiting on a queue that's not running because of an
  unexpected signal). Example: There are still very rare edge cases in
  Elfeed I haven't caught (I suspect there's one related to DNS failures),
  and once every few months or so I have to use the emergency elfeed-unjam
  lever to reset the queue to a good state.

  plist-put is destructive, but you must still capture the return value,
  particularly when the property doesn't exist yet. (Imagine setting a
  property on a nil list.)

  Is "delete" the right function for removing items? This uses "equal" but
  you probably want eq like delq. cl-delete more sensibly uses "eql" by
  default, which is just as good.

  Some nit-picky stuff that probably doesn't matter, but I can't help
  commenting since I'm (overly) sensitive to pessimization:

  ,* Using "delete" on the active list is (almost) quadratic time, or more
  accurately, O(n*m) for n requests and a limit of m. Using a set (read:
  hashtable) or some other kind of O(1) removal would bring this down to
  linear, O(n), time. I'm putting this under nit-picking since the limit is
  likely a small number.

  ,* The use of "length" in plz-run is O(n*m) time just like delete. You can
  avoid this by tracking the length as separate counter rather than walking
  the list in order to count. Alternatively, this is automatically fixed
  when you swap in a set, since presumably it has an O(1) length function.

  ,* Similarly, I don't like the recursion in plz-run even though it's also
  bounded by the limit. Unless Emacs got TCO when I wasn't looking, I'd a
  loop just to be more careful. (Go recently ran into trouble parsing PEM
  using recursion despite having massive call stacks.)

  Ending on a positive note: I like that you consistently return the queue
  object. (Except for plz-run?)
#+end_example

* API Design

** Async

Some sample cases that the API should make easy.

*** Body as string

#+BEGIN_SRC elisp
  (plz-get url
    :with 'body-string
    :then (lambda (body-string)
            (setf something body-string)))
#+END_SRC

*** Body as buffer

#+BEGIN_SRC elisp
  ;; Decodes body and narrows buffer to it.
  (plz-get url
    :with 'buffer
    :then (lambda (buffer)
            (with-current-buffer buffer
              (setf text (buffer-substring (point-min) (point-max))))))
#+END_SRC

#+BEGIN_SRC elisp
  ;; Narrows buffer to undecoded body, e.g. for binary files.
  (plz-get url
    :with 'buffer-undecoded  ; `buffer-binary'?
    :then (lambda (buffer)
            (with-current-buffer buffer
              (setf binary-content (buffer-substring (point-min) (point-max))))))
#+END_SRC

**** Callback with point at body start
:PROPERTIES:
:ID:       1795462e-01bc-4f0b-97ab-3c1b2e75485c
:END:

Assuming that =plz= has already called =decode-coding-region=, this is straightforward, but the caller shouldn't have to do this extra work.

#+BEGIN_SRC elisp
  (plz-get url
    :then (lambda (buffer)
            (buffer-substring (point) (point-max))))
#+END_SRC

*** Body parsed with function

#+BEGIN_SRC elisp
  ;; Narrows buffer to body, decodes it, calls callback with result of `json-read'.
  (plz-get url
    :with #'json-read
    :then (lambda (json)
            (setf something (alist-get 'key json))))
#+END_SRC

#+BEGIN_SRC elisp
  ;; Narrows buffer to body, decodes it, parses with
  ;; `libxml-parse-html-region', calls callback with DOM.
  (plz-get url
    :with (lambda ()
            (libxml-parse-html-region (point-min) (point-max) url))
    :then (lambda (dom)
            (with-current-buffer (generate-new-buffer "*plz-browse*")
              (shr-insert-document dom))))
#+END_SRC

*** HTTP response with headers

* References
:PROPERTIES:
:TOC:      :depth 1
:END:

** Users

Known users of =plz=.

+ [[https://github.com/alphapapa/ement.el][GitHub - alphapapa/ement.el: Matrix client for Emacs]]
+ [[https://git.sr.ht/~ushin/hyperdrive.el][~ushin/hyperdrive.el - Emacs gateway to the Hypercore network - sourcehut git]]
+ [[https://github.com/Fuco1/org-node-graph/blob/master/org-graph-embeddings.el][org-node-graph]]
+ [[https://github.com/jinnovation/kele.el][GitHub - jinnovation/kele.el: 🥤 Spritzy Kubernetes cluster management for Emacs]]
+ [[https://github.com/purplg/orrient.el][GitHub - purplg/orrient.el]]
+ [[https://sr.ht/~akagi/srht.el/][srht.el: Emacs sr.ht API client]]
+ [[https://github.com/merrickluo/qbittorrent.el][GitHub - merrickluo/qbittorrent.el: A qBittorrent client for Emacs.]]
+ [[https://github.com/akirak/emacs-dumb-japanese][GitHub - akirak/emacs-dumb-japanese: (Experimental) An opinionated Japanese input method that never learns your language]]
+ ...and others that I don't recall at the moment...
  
** Discussions

*** [[https://lists.gnu.org/archive/html/emacs-devel/2023-03/msg00430.html][continuation passing in Emacs vs. JUST-THIS-ONE]]
:PROPERTIES:
:ID:       bbf01f92-1a55-4b86-a92b-f7ef0ed6ad4a
:END:

Later discussion in the thread mentions using timers to improve responsiveness, and I wonder if something like that would be useful in ~plz~ (e.g. having the process sentinel just call ~(run-at-time nil ...)~ and do all the work in a function called by the timer--maybe it would make error handling easier or cleaner or less troublesome by not doing so in the sentinel).

*** [[https://lists.gnu.org/archive/html/emacs-devel/2016-12/msg01070.html][with-url]]

[2020-12-20 Sun 08:11]  At the end of 2016, Lars Ingebrigtsen [[https://lists.gnu.org/archive/html/emacs-devel/2016-12/msg01070.html][proposed]] a ~with-url~ macro that improves on ~url-retrieve~ and ~url-retrieve-synchronously~.  It was [[https://lists.gnu.org/archive/html/emacs-devel/2020-12/msg01220.html][mentioned]] by David Engster in [[https://lists.gnu.org/archive/html/emacs-devel/2020-12/msg01217.html][this thread]] from 2020.  It looks like it has a nice API.  Unfortunately it hasn't been merged.

*** TODO Feedback from Chris Wellons :ATTACH:
:PROPERTIES:
:ID:       975f77fa-5233-4b26-970b-e0d64f2aa950
:Attachments: https%3A%2F%2Fgithub.com%2Falphapapa%2Fplz.el%2Fcommit%2F0a860d94dcbb103d05f3ee006772a568904fa4de%23commitcomment-55151841--n1jasq.tar.xz https%3A%2F%2Fgithub.com%2Falphapapa%2Fplz.el%2Fcommit%2F7c27e4bdcd747f0bfc5a6298040739562a941e08%23r55075010--WNr6Ey.tar.xz
:END:
:LOGBOOK:
-  State "TODO"       from              [2021-08-20 Fri 05:37]
:END:

+ [[https://github.com/alphapapa/plz.el/commit/7c27e4bdcd747f0bfc5a6298040739562a941e08#r55075010][Change: Sync with accept-process-output · alphapapa/plz.el@7c27e4b · GitHub]]
+ [[https://github.com/alphapapa/plz.el/commit/0a860d94dcbb103d05f3ee006772a568904fa4de#commitcomment-55151841][Tests: "There be dragons." · alphapapa/plz.el@0a860d9 · GitHub]]

**** [[https://github.com/alphapapa/plz.el/commit/3469bcdbb2e2c1a772ecadcf4f50da317065a96d][WIP: Queueing · alphapapa/plz.el@3469bcd · GitHub]]

#+begin_src markdown
  Going from 0 dependencies to 1 dependency is a big jump. With zero you get
  some nice benefits, like never worrying about a package system (esp. when
  testing, etc.). If you're going to make that transition it better be worth
  the cost. A linked list queue is not worth it.

  While that ELPA queue package is decent enough, this is trivial
  functionality. You can implement an equivalent queue in just a few lines
  of code. For example, here's a minimalistic one built out of a cons:

  https://github.com/skeeto/emacs-aio/blob/master/aio.el#L322

  (Feel free to copy this one if you like. It's in the public domain, and I
  don't even care if you give me credit since it's so trivial. "A little
  copying is better than a little dependency.")

  plz-clear and plz-reset: Don't immediately nil the active list. A request
  is not properly retired until all the callbacks have completed, and
  requests should remain in the active list until then. That probably also
  means blocking clear/reset until the active list clears. Otherwise 1) the
  caller might observe an empty queue with invisible still-active requests,
  which isn't really a valid state, and 2) it's up to the caller to somehow
  wait (complicated and error-prone) if needed for the queue to return to a
  valid state. You probably need to track this "cancellation" state so that
  your wrapper callbacks don't actually run the queue, and so you can
  potentially catch/handle callbacks enqueueing while you're busy trying to
  clear the active list.

  For plz-clear: queued, inactive requests should also have their :else
  callbacks invoked to indicate they're not going to happen. Perhaps some
  kind of "cancelled" error?

  For plz-reset: I'm not sure this is really even a sound idea. The request
  is concurrent, and it may still complete on the server side, including any
  side effects, despite killing the process. These requests should not be
  retried unless the caller explicitly requeues them (they know the context
  but plz does not), and they'd know to do so because you reported to :else
  that it was canceled.

  I've probably said it before, but rigid guarantees around callbacks are
  super important. In order to build anything reliable on this, callbacks
  must be called exactly the right number of times (i.e. exactly once, not
  zero or twice) at the right time, and the invariants must hold around
  these callbacks (i.e. the queue state makes sense during the callback).
  Fundamentally this is a concurrency situation even if there are no
  explicit threads/coroutines involved. The biggest flaw with Emacs' own
  url-retrieve, and the primary reason it's so unreliable, is its poor,
  unpredictable callback discipline.

  So a rule: When something goes in the queue, it stays there until plz has
  informed its callbacks of the results. The callbacks on an enqueued
  request are never called twice (for that request): it either fails or
  succeeds and that's it.

  There's a warning about signals in callbacks aborting queue processing,
  but I'd just make the queue robust regardless. Let the signal unwind to
  the top-level and make noise, but keep the queue moving. A mistake will
  eventually happen, and then some consumer of this library will end up in
  an invalid state (e.g. waiting on a queue that's not running because of an
  unexpected signal). Example: There are still very rare edge cases in
  Elfeed I haven't caught (I suspect there's one related to DNS failures),
  and once every few months or so I have to use the emergency elfeed-unjam
  lever to reset the queue to a good state.

  plist-put is destructive, but you must still capture the return value,
  particularly when the property doesn't exist yet. (Imagine setting a
  property on a nil list.)

  Is "delete" the right function for removing items? This uses "equal" but
  you probably want eq like delq. cl-delete more sensibly uses "eql" by
  default, which is just as good.

  Some nit-picky stuff that probably doesn't matter, but I can't help
  commenting since I'm (overly) sensitive to pessimization:

  ,* Using "delete" on the active list is (almost) quadratic time, or more
  accurately, O(n*m) for n requests and a limit of m. Using a set (read:
  hashtable) or some other kind of O(1) removal would bring this down to
  linear, O(n), time. I'm putting this under nit-picking since the limit is
  likely a small number.

  ,* The use of "length" in plz-run is O(n*m) time just like delete. You can
  avoid this by tracking the length as separate counter rather than walking
  the list in order to count. Alternatively, this is automatically fixed
  when you swap in a set, since presumably it has an O(1) length function.

  ,* Similarly, I don't like the recursion in plz-run even though it's also
  bounded by the limit. Unless Emacs got TCO when I wasn't looking, I'd a
  loop just to be more careful. (Go recently ran into trouble parsing PEM
  using recursion despite having massive call stacks.)

  Ending on a positive note: I like that you consistently return the queue
  object. (Except for plz-run?)
#+end_src

*** [[https://lists.gnu.org/archive/html/emacs-devel/2010-09/msg00199.html][Blocking call to accept-process-output with quit inhibited!! [11 times]​]]
:PROPERTIES:
:ID:       57a9bbd4-9b6e-4db0-a293-a099d4a05c89
:END:

This thread discusses how process sentinels, et al inhibit quit, which causes that message to be displayed.

Some quotes from Stefan:

#+begin_quote
No, usually it's there because the Elisp coder is not aware of the risk (e.g. he doesn't realize his code will be run with inhibit-quit set, probably because he doesn't realize that this is set whenever we run process filters, process sentinels, post-command-hook, jit-lock, timers, and a handful other cases).
#+end_quote

#+begin_quote
> But why then is quit inhibited at all?

Because these are run asynchronously, so in most cases the user has no idea that code is being executed at that point.  So if she hits C-g it's likely to be for other reasons than to interrupt the async code, so by default Emacs runs the async code to completion first and then processes the C-g (you wouldn't want a C-g meant to abort a command to result in a half-font-locked display, would you?).  I.e. if you want to execute code that may last for a significant amount of time from one of those places, you need with-local-quit (and you may want to make sure the user knows that such code is being run).
#+end_quote

*** About request.el

**** [[https://github.com/alphapapa/matrix-client.el/pull/27#issuecomment-428689369][Early WIP: "next-gen" client by alphapapa · Pull Request #27 · alphapapa/matrix-client.el · GitHub]]

A comment I made on [2018-10-10 Wed]:

#+begin_quote markdown
@jgkamat Okay, well, now I remember what the problem with `request` is: sync requests start getting duplicated.  The last message I received in a room arrived 21 times!  When I first run `matrix-client-ng-connect`, it's fine.  But over time, sync requests start getting duplicated.  I don't know how it would be possible for that to happen, but it is happening.

And I don't know where to begin to debug it.  Digging through the log buffer to find where a sync request is first duplicated is tedious and error-prone.  Even if I found where, it wouldn't tell me why it happened.  And `request` is labyrinthine and very difficult to follow by stepping through with edebug.  And this never happens with `url`.

I guess our best option now is to keep using `url-with-retrieve-async` for everything that works for, and use `request` only for HTTP requests that don't work with `url`.  Sigh.  What a mess.
#+end_quote

*** [[https://github.com/tkf/emacs-request/issues/207][Sending should not rely on UNIX pipes -- it's slow. · Issue #207 · tkf/emacs-request · GitHub]]
:PROPERTIES:
:ID:       c0012a30-0d08-4b48-8d3a-89d1f3deec20
:END:

This issue notes that sending large files (e.g. 25 MB) to curl via UNIX pipes is slow.

** Bug reports

*** Emacs Process-related bugs

**** [[https://debbugs.gnu.org/cgi/bugreport.cgi?bug=50166][#50166 - 28.0.50; ECM for possible process-status/sentinel bug - GNU bug report logs]]

**** [[https://debbugs.gnu.org/cgi/bugreport.cgi?bug=49897][#49897 - 28.0.50; {PATCH} Make sense of url-retrieve-synchronously - GNU bug report logs]]

**** [[https://debbugs.gnu.org/cgi/bugreport.cgi?bug=49682][#49682 - 27.2.50; accept-process-output within accept-process-output hangs emacs - GNU bug report logs]]

**** [[https://debbugs.gnu.org/cgi/bugreport.cgi?bug=33018][#33018 - 26.1.50; thread starvation with async processes and accept-process-output - GNU bug report logs]]

**** [[https://debbugs.gnu.org/cgi/bugreport.cgi?bug=24201][#24201 - 25.1.50; TLS connections sometimes hang - GNU bug report logs]]

**** [[https://debbugs.gnu.org/cgi/bugreport.cgi?bug=20159][#20159 - 24.4; url-retrieve invokes same callback twice with kill-buffer - GNU bug report logs]]

Chris Wellons file this report on 21 Mar 2015.

[[https://debbugs.gnu.org/cgi/bugreport.cgi?bug=20159#43][This comment]] by Lars seems especially relevant:

#+begin_quote
I've done some further debugging, and what's happening is (simply, ahem)
that when we get a "connection failed" message from the sentinel, and
then kill the buffer, we also kill the process, and then the sentinel is
called again, and then it calls our callback again.

So the weird thing is that the process isn't already dead, kinda...  Or
something...
#+end_quote

As well as [[https://debbugs.gnu.org/cgi/bugreport.cgi?bug=20159#48][the following one]]:

#+begin_quote
The two statuses are these:

Process status failed
Process status closed

But I kinda think that url.el is behaving as specified.  The callbacks
may be called many times, and you're supposed to check the status.  And
if you're killing the same buffer twice, this is pilot error.

So I don't think there's really a bug here.
#+end_quote

** Similar projects

*** [[https://github.com/ebpa/fetch.el][GitHub - ebpa/fetch.el: A simple HTTP request library modeled after the web browser API]]

A nice-looking wrapper for =url.el=.


